// app.js
// Neural Network Design: The Gradient Puzzle
// ------------------------------------------------------------
// –ì–û–¢–û–í–´–ô –ö–û–î: —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç custom loss,
// –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º –≤ –ø–ª–∞–≤–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç.
// –†–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏. –ú–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã.
// ------------------------------------------------------------

// ---------- Configuration & Constants ----------
const INPUT_SIZE = 16;           // 16x16 grayscale
const LATENT_COMPRESS = 64;      // compression bottleneck
const LATENT_TRANSFORM = 256;    // transformation (same as input)
const LATENT_EXPAND = 512;       // expansion bottleneck

// Fixed random input (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —à—É–º –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏)
const xInput = tf.tidy(() => tf.randomUniform([1, INPUT_SIZE, INPUT_SIZE, 1], 0, 1, 'float32', 42));

// UI Elements
const canvasInput = document.getElementById('canvasInput');
const canvasBaseline = document.getElementById('canvasBaseline');
const canvasStudent = document.getElementById('canvasStudent');
const logDiv = document.getElementById('logContent');
const stepSpan = document.getElementById('stepCount');

// State
let baselineModel, studentModel;
let studentOptimizer = tf.train.adam(0.01);
let baselineOptimizer = tf.train.adam(0.01);
let step = 0;
let autoTrainInterval = null;
let currentArch = 'compression';   // default

// ---------- Loss components (provided) ----------

function mse(yTrue, yPred) {
  return tf.losses.meanSquaredError(yTrue, yPred).mean();
}

// Sorted MSE (quantile / wasserstein) ‚Äì liberates pixels from positions
function sortedMSE(yTrue, yPred) {
  return tf.tidy(() => {
    const flatTrue = yTrue.flatten();
    const flatPred = yPred.flatten();
    const size = flatTrue.shape[0];
    const sortedTrue = tf.topk(flatTrue, size).values;
    const sortedPred = tf.topk(flatPred, size).values;
    return tf.losses.meanSquaredError(sortedTrue, sortedPred).mean();
  });
}

// Smoothness (total variation) ‚Äì encourages local consistency
function smoothness(yPred) {
  return tf.tidy(() => {
    const left = yPred.slice([0,0,0,0], [-1, INPUT_SIZE-1, -1, -1]);
    const right = yPred.slice([0,0,1,0], [-1, INPUT_SIZE-1, -1, -1]);
    const dh = right.sub(left).square().mean();
    const top = yPred.slice([0,0,0,0], [-1, INPUT_SIZE-1, -1, -1]);
    const bottom = yPred.slice([0,1,0,0], [-1, INPUT_SIZE-1, -1, -1]);
    const dv = bottom.sub(top).square().mean();
    return dh.add(dv).div(tf.scalar(2));
  });
}

// Direction loss: bright on right, dark on left
function directionX(yPred) {
  return tf.tidy(() => {
    const weights = tf.linspace(0, 1, INPUT_SIZE).reshape([1, 1, INPUT_SIZE]);
    const weightMatrix = weights.tile([INPUT_SIZE, 1]).reshape([1, INPUT_SIZE, INPUT_SIZE, 1]);
    const weighted = yPred.mul(weightMatrix).mean();
    // –º—ã —Ö–æ—Ç–∏–º –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å weighted -> –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º -weighted
    return tf.scalar(-1).mul(weighted);
  });
}

// ---------- Model creators ----------

// Baseline model (fixed, MSE only)
function createBaselineModel() {
  const model = tf.sequential();
  model.add(tf.layers.flatten({ inputShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
  model.add(tf.layers.dense({ units: 128, activation: 'relu' }));
  model.add(tf.layers.dense({ units: 256, activation: 'sigmoid' }));
  model.add(tf.layers.reshape({ targetShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
  return model;
}

// Student model ‚Äì architecture depends on selection
function createStudentModel(archType) {
  if (archType === 'compression') {
    const model = tf.sequential();
    model.add(tf.layers.flatten({ inputShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
    model.add(tf.layers.dense({ units: 128, activation: 'relu' }));
    model.add(tf.layers.dense({ units: LATENT_COMPRESS, activation: 'relu' }));
    model.add(tf.layers.dense({ units: 256, activation: 'sigmoid' }));
    model.add(tf.layers.reshape({ targetShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
    return model;
  }
  else if (archType === 'transformation') {
    // ----- transformation (bottleneck = 256, same as flattened 256) -----
    const model = tf.sequential();
    model.add(tf.layers.flatten({ inputShape: [INPUT_SIZE, INPUT_SIZE, 1] })); // 256
    model.add(tf.layers.dense({ units: 256, activation: 'relu' }));
    model.add(tf.layers.dense({ units: 256, activation: 'relu' })); // no compression
    model.add(tf.layers.dense({ units: 256, activation: 'sigmoid' }));
    model.add(tf.layers.reshape({ targetShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
    return model;
  }
  else if (archType === 'expansion') {
    // ----- expansion (bottleneck wider: 512) -----
    const model = tf.sequential();
    model.add(tf.layers.flatten({ inputShape: [INPUT_SIZE, INPUT_SIZE, 1] })); // 256
    model.add(tf.layers.dense({ units: 512, activation: 'relu' }));
    model.add(tf.layers.dense({ units: LATENT_EXPAND, activation: 'relu' })); // 512
    model.add(tf.layers.dense({ units: 256, activation: 'sigmoid' }));
    model.add(tf.layers.reshape({ targetShape: [INPUT_SIZE, INPUT_SIZE, 1] }));
    return model;
  }
  throw new Error(`Unknown architecture: ${archType}`);
}

// ---------- CUSTOM LOSS (already tuned for gradient emergence) ----------
function studentLoss(yTrue, yPred) {
  // –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è: sortedMSE —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫—É –ø–∏–∫—Å–µ–ª–µ–π,
  // smoothness —É–±–∏—Ä–∞–µ—Ç —à—É–º, direction —Å–æ–∑–¥–∞—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç.
  // –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ø–æ–¥–æ–±—Ä–∞–Ω—ã —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏.
  const sortedVal = sortedMSE(yTrue, yPred);
  const smoothVal = smoothness(yPred);
  const dirVal = directionX(yPred);

  // –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π mse, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–±—â—É—é —è—Ä–∫–æ—Å—Ç—å,
  // –Ω–æ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ mse –º–æ–∂–Ω–æ –æ–±–Ω—É–ª–∏—Ç—å.
  return sortedVal * 10.0 + smoothVal * 2.0 + dirVal * 5.0;
}

// ---------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ----------
function initModels() {
  tf.tidy(() => {
    if (baselineModel) baselineModel.dispose();
    if (studentModel) studentModel.dispose();
    baselineModel = createBaselineModel();
    // —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞—ë—Ç—Å—è –ø–æ —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
    studentModel = createStudentModel(currentArch);
    studentOptimizer = tf.train.adam(0.01);
    baselineOptimizer = tf.train.adam(0.01);
  });
  step = 0;
  log(`üîÑ –º–æ–¥–µ–ª–∏ —Å–±—Ä–æ—à–µ–Ω—ã, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: ${currentArch}`);
  updateLogAndCanvas();
}

// ---------- –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π trainStep —Å GradientTape ----------
function trainStep() {
  // –ò—Å–ø–æ–ª—å–∑—É–µ–º tf.tidy –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
  tf.tidy(() => {
    // ---- Baseline (MSE only) ----
    const baselineVars = baselineModel.trainableVariables;
    const baselineLoss = tf.tidy(() => {
      const pred = baselineModel.predict(xInput);
      return mse(xInput, pred);
    });
    // –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã baseline
    const baselineGrads = tf.grads(loss => loss)(baselineLoss, baselineVars);
    baselineOptimizer.applyGradients(baselineGrads);
    // –æ—á–∏—Å—Ç–∫–∞ (tf.tidy —Å–¥–µ–ª–∞–µ—Ç –≤—Å—ë —Å–∞–º, –Ω–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã)

    // ---- Student (custom loss) ----
    const studentVars = studentModel.trainableVariables;
    let studentLossValue;
    const studentGrads = tf.variableGrads(() => {
      const pred = studentModel.predict(xInput);
      const loss = studentLoss(xInput, pred);
      studentLossValue = loss.clone(); // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
      return loss;
    });
    studentOptimizer.applyGradients(studentGrads.grads);
    // –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –≥—Ä–∞—Ñ–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (studentGrads —Å–∞–º –æ—á–∏—Å—Ç–∏—Ç—Å—è –≤ tidy)

    // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    step++;
    const baselineLossVal = baselineLoss.dataSync()[0];
    const studentLossVal = studentLossValue.dataSync()[0];
    log(`step ${step} | baseline ${baselineLossVal.toFixed(4)} | student ${studentLossVal.toFixed(4)}`);
  });
  updateCanvas();
}

// ---------- –û—Ç—Ä–∏—Å–æ–≤–∫–∞ ----------
function renderTensorToCanvas(tensor, canvas) {
  tf.tidy(() => {
    const imgData = tensor.squeeze([0]); // [16,16,1]
    tf.browser.toPixels(imgData, canvas).catch(e => console.warn('canvas render error', e));
  });
}

function updateCanvas() {
  renderTensorToCanvas(xInput, canvasInput);
  if (baselineModel) {
    const pred = baselineModel.predict(xInput);
    renderTensorToCanvas(pred, canvasBaseline);
    pred.dispose();
  }
  if (studentModel) {
    const pred = studentModel.predict(xInput);
    renderTensorToCanvas(pred, canvasStudent);
    pred.dispose();
  }
}

function updateLogAndCanvas() {
  updateCanvas();
  stepSpan.innerText = `step ${step}`;
}

function log(msg) {
  logDiv.innerText = msg;
  stepSpan.innerText = `step ${step}`;
}

// ---------- UI –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ ----------
document.getElementById('trainStepBtn').addEventListener('click', () => {
  trainStep();
  updateLogAndCanvas();
});

document.getElementById('autoTrainBtn').addEventListener('click', (e) => {
  if (autoTrainInterval) {
    clearInterval(autoTrainInterval);
    autoTrainInterval = null;
    e.target.innerText = '‚ñ∂ Auto Train (Start)';
  } else {
    autoTrainInterval = setInterval(() => {
      trainStep();
      updateLogAndCanvas();
    }, 80); // ~12 —à–∞–≥–æ–≤/—Å–µ–∫
    e.target.innerText = '‚è∏ Auto Train (Stop)';
  }
});

document.getElementById('resetBtn').addEventListener('click', () => {
  if (autoTrainInterval) {
    clearInterval(autoTrainInterval);
    autoTrainInterval = null;
    document.getElementById('autoTrainBtn').innerText = '‚ñ∂ Auto Train (Start)';
  }
  initModels();
  updateLogAndCanvas();
});

// –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å—Ç—É–¥–µ–Ω—Ç–∞
document.querySelectorAll('input[name="arch"]').forEach(radio => {
  radio.addEventListener('change', (e) => {
    currentArch = e.target.value;
    // –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    if (studentModel) studentModel.dispose();
    studentModel = createStudentModel(currentArch);
    studentOptimizer = tf.train.adam(0.01); // —Å–≤–µ–∂–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    log(`üîÅ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å—Ç—É–¥–µ–Ω—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ ${currentArch}`);
    updateCanvas();
  });
});

// ---------- –ó–∞–ø—É—Å–∫ ----------
initModels();
log('üöÄ –≥–æ—Ç–æ–≤–æ. student loss = sortedMSE*10 + smoothness*2 + direction*5. –ù–∞–∂–∏–º–∞–π—Ç–µ Train!');
updateLogAndCanvas();
